{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14495532,"sourceType":"datasetVersion","datasetId":9220946}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nBASE_DIR = Path(\".\")\n\n\ndef check_env() -> str:\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n        print(\"Running on Kaggle\")\n        return \"kaggle\"\n    else:\n        print(\"Running locally\")\n        return \"local\"\n\n\nENV = check_env()\n\nif ENV == \"kaggle\":\n    data_dir = Path(\"/kaggle/input/ka-ocr\")\nelse:\n    data_dir = BASE_DIR / \"data\"\n\nprint(f\"\\nDataset contents in {data_dir}:\")\nfor item in data_dir.iterdir():\n    print(f\"{item.name}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:50.759649Z","iopub.execute_input":"2026-01-24T06:08:50.759996Z","iopub.status.idle":"2026-01-24T06:08:50.771919Z","shell.execute_reply.started":"2026-01-24T06:08:50.759936Z","shell.execute_reply":"2026-01-24T06:08:50.771214Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:09.346660Z","start_time":"2026-01-23T18:56:09.338928Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle\n\nDataset contents in /kaggle/input/ka-ocr:\nbpg_phone_sans_bold_italic\nbpg_glaho_bold\nbpg_irubaqidze\nbpg_boxo-boxo\nbpg_glaho_2008\nbpg_paata_cond\nmg_bitneon_chaos\nliteraturulitt\nversion.txt\nar-archy-regular\nbpg_paata_caps\nmg_bitneon\nfixedsys_excelsior\nka_literaturuli\ngugeshashvili_slfn_2\nbpg_glaho\nbpg_dedaena\nNotoSansGeorgian\ngf_aisi_nus-bold-italic\narial_geo-bold-italic\narial_geo-italic\nbpg_sans_2008\nbpg_nostalgia\nbpg_supersquare_2009\narial_geo\nbpg_phone_sans_bold\nalkroundednusx-medium\ngf_aisi_nus_medium-medium-italic\nbpg_phone_sans_italic\nbpg_excelsior_caps_dejavu_2010\nbpg_nino_elite_ultra\nbpg_rioni_contrast\narial_geo-bold\nbpg_no9\n3d_unicode\nbpg_dedaena_nonblock\nbpg_extrasquare_2009\nbpg_algeti_compact\nka_lortkipanidze\nalkroundedmtav-medium\nbpg_nino_mtavruli_bold\nbpg_glaho_sylfaen\nbpg_mrgvlovani_caps_2010\nbpg_nino_elite_exp\nbpg_ucnobi\nbpg_arial_2009\nbpg_boxo\nbpg_glaho_arial\nbpg_rioni_vera\nbpg_paata_cond_caps\nbpg_nino_mtavruli_book\nbpg_paata_exp\nbpg_classic_medium\nbpg_rioni\nbpg_excelsior_dejavu_2010\nmetadata.csv\nbpg_extrasquare_mtavruli_2009\nbpg_nino_mtavruli_normal\nmg_niniko\nbpg_quadrosquare_2009\nbpg_nino_elite_ultra_caps\nbpg_venuri_2010\nbpg_serif_2008\nbpg_square_2009\nbpg_nino_medium_caps\nbpg_glaho_traditional\nbpg_paata\nbpg_algeti\nbpg_ingiri_2008\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Explore data","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:50.773360Z","iopub.execute_input":"2026-01-24T06:08:50.773630Z","iopub.status.idle":"2026-01-24T06:08:51.077798Z","shell.execute_reply.started":"2026-01-24T06:08:50.773603Z","shell.execute_reply":"2026-01-24T06:08:51.077183Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:09.415841Z","start_time":"2026-01-23T18:56:09.411884Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(data_dir/\"metadata.csv\")\nprint(df.head())\nprint(df.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:51.078753Z","iopub.execute_input":"2026-01-24T06:08:51.079129Z","iopub.status.idle":"2026-01-24T06:08:51.280758Z","shell.execute_reply.started":"2026-01-24T06:08:51.079093Z","shell.execute_reply":"2026-01-24T06:08:51.279910Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:09.595294Z","start_time":"2026-01-23T18:56:09.432612Z"}},"outputs":[{"name":"stdout","text":"                        file_name      text\n0  3d_unicode/3d_unicode_0000.png        ·É†·Éê\n1  3d_unicode/3d_unicode_0001.png  ·É¨·Éß·Éì·Éî·Éë·Éù·Éì·Éê\n2  3d_unicode/3d_unicode_0002.png   ·É≠·ÉØ·Éú·É¨·É§·É£·É•\n3  3d_unicode/3d_unicode_0003.png    ·É¨·Éõ·Éò·Éú·Éì·Éê\n4  3d_unicode/3d_unicode_0004.png     ·ÉØ·Éò·Éö·Éì·Éù\n                                         file_name         text\n100495  NotoSansGeorgian/NotoSansGeorgian_1495.png     ·ÉÆ·Éó·Éï·É∞·Éü·É®·ÉÆ·É®\n100496  NotoSansGeorgian/NotoSansGeorgian_1496.png       ·É†·Éù·Éõ·Éö·Éò·É°\n100497  NotoSansGeorgian/NotoSansGeorgian_1497.png  ·É®·Éî·Éõ·Éù·É†·É©·Éî·Éú·Éò·Éö·Éò\n100498  NotoSansGeorgian/NotoSansGeorgian_1498.png    ·Éú·Éî·Éù·Éö·Éò·Éó·É£·É†·Éò\n100499  NotoSansGeorgian/NotoSansGeorgian_1499.png         ·É£·É™·ÉÆ·Éù\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(df[\"text\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:51.281894Z","iopub.execute_input":"2026-01-24T06:08:51.282514Z","iopub.status.idle":"2026-01-24T06:08:51.315500Z","shell.execute_reply.started":"2026-01-24T06:08:51.282486Z","shell.execute_reply":"2026-01-24T06:08:51.314524Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:09.640649Z","start_time":"2026-01-23T18:56:09.612085Z"}},"outputs":[{"name":"stdout","text":"text\n·Éì·Éê              4205\n·Éê·É†              1155\n·É†·Éù·Éõ             1044\n·Éò·Éß·Éù              785\n·Éô·Éò               612\n                ... \n·Éò·É§·É®·Éï·Éú·Éî·É¢·É°           1\n·Éì·Éê·Éí·Éï·Éî·ÉÆ·Éê·É†·ÉØ·Éù·É°        1\n·É•·É†·Éù·Éõ·Éê·É¢·Éò·Éì·Éî·Éë·Éò·É°       1\n·Éõ·Éê·Éí·É†·Éê·Éó·Éê            1\n·É®·É®·É•·É¢·Éî·É®             1\nName: count, Length: 37994, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Check text length variations\ndf[\"text_len\"] = df[\"text\"].str.len()\nprint(df[\"text_len\"].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:51.317706Z","iopub.execute_input":"2026-01-24T06:08:51.318341Z","iopub.status.idle":"2026-01-24T06:08:51.363511Z","shell.execute_reply.started":"2026-01-24T06:08:51.318314Z","shell.execute_reply":"2026-01-24T06:08:51.362557Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:09.721697Z","start_time":"2026-01-23T18:56:09.679014Z"}},"outputs":[{"name":"stdout","text":"count    100500.000000\nmean          6.396020\nstd           2.979581\nmin           2.000000\n25%           4.000000\n50%           6.000000\n75%           8.000000\nmax          30.000000\nName: text_len, dtype: float64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Prepare dataset and tokenizer","metadata":{}},{"cell_type":"markdown","source":"Checking if trocr already support tokenization for Georgian","metadata":{}},{"cell_type":"code","source":"from transformers import TrOCRProcessor\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n\n# Test Georgian tokenization\ntest_text = \"·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\"\ntokens = processor.tokenizer.tokenize(test_text)\nprint(tokens)  # If you see lots of <unk> or weird splits, you need a custom tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:08:51.364693Z","iopub.execute_input":"2026-01-24T06:08:51.365460Z","iopub.status.idle":"2026-01-24T06:09:19.260228Z","shell.execute_reply.started":"2026-01-24T06:08:51.365431Z","shell.execute_reply":"2026-01-24T06:09:19.259422Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:11.988100Z","start_time":"2026-01-23T18:56:09.738192Z"}},"outputs":[{"name":"stderr","text":"2026-01-24 06:08:59.891142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769234940.097086      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769234940.161816      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769234940.653243      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769234940.653306      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769234940.653309      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769234940.653312      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88cc416c7a745419e6378e53d6d1dc4"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d1401df304e4924b959fa97cd4b3517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab857b7b470459fbae3d7c0db24c18e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5aa34a2b09a4789ba4ee971f42735d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f29a4593ba9d460692f0ced52ea876d2"}},"metadata":{}},{"name":"stdout","text":"['√°', 'ƒ•', 'ƒ¥', '√°', 'ƒ•', 'ƒ≤', '√°', 'ƒ•', 'ƒΩ', '√°', 'ƒ•', 'ƒ≤', '√°', 'ƒ•', '≈Ç', '√°', 'ƒ•', '¬Ø', '√°', 'ƒ•', 'ƒø', '√°', 'ƒ•', 'ƒ≥', '√°', 'ƒ•', 'ƒ≤']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"That's not what we need, so we'll create custom, character-based tokenizer.\n\nThe model predicts next token, in this case token represents char, not a word.","metadata":{}},{"cell_type":"code","source":"class GeorgianTokenizer:\n    def __init__(self, max_length: int = 32):\n        # Special tokens\n        self.pad_token = \"<pad>\"\n        self.bos_token = \"<s>\"      # beginning of sequence\n        self.eos_token = \"</s>\"     # end of sequence\n        self.unk_token = \"<unk>\"    # unknown character\n\n        # Georgian alphabet (33 letters)\n        self.georgian_chars = \"·Éê·Éë·Éí·Éì·Éî·Éï·Éñ·Éó·Éò·Éô·Éö·Éõ·Éú·Éù·Éû·Éü·É†·É°·É¢·É£·É§·É•·É¶·Éß·É®·É©·É™·É´·É¨·É≠·ÉÆ·ÉØ·É∞\"\n\n        # Build vocabulary: special tokens + Georgian characters\n        self.vocab = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n        self.vocab.extend(list(self.georgian_chars))\n\n        # Create mappings\n        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n\n        # Token IDs for special tokens\n        self.pad_token_id = 0\n        self.bos_token_id = 1\n        self.eos_token_id = 2\n        self.unk_token_id = 3\n\n        self.max_length = max_length\n\n    def encode(self, text: str, padding: bool = True) -> list[int]:\n        \"\"\"Convert Georgian text to token IDs.\"\"\"\n        # Start with BOS token\n        ids = [self.bos_token_id]\n\n        # Convert each character\n        for char in text:\n            ids.append(self.char_to_id.get(char, self.unk_token_id))\n\n        # Add EOS token\n        ids.append(self.eos_token_id)\n\n        # Truncate if too long\n        if len(ids) > self.max_length:\n            ids = ids[:self.max_length - 1] + [self.eos_token_id]\n\n        # Pad if needed\n        if padding:\n            ids.extend([self.pad_token_id] * (self.max_length - len(ids)))\n\n        return ids\n\n    def decode(self, ids: list[int]) -> str:\n        \"\"\"Convert token IDs back to text.\"\"\"\n        chars = []\n        for id in ids:\n            if id in (self.pad_token_id, self.bos_token_id, self.eos_token_id):\n                continue\n            chars.append(self.id_to_char.get(id, \"\"))\n        return \"\".join(chars)\n\n    def __len__(self):\n        return len(self.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:19.261418Z","iopub.execute_input":"2026-01-24T06:09:19.262468Z","iopub.status.idle":"2026-01-24T06:09:19.271144Z","shell.execute_reply.started":"2026-01-24T06:09:19.262425Z","shell.execute_reply":"2026-01-24T06:09:19.270480Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:12.196593Z","start_time":"2026-01-23T18:56:12.189134Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Test tokenization again","metadata":{}},{"cell_type":"code","source":"tokenizer = GeorgianTokenizer(max_length=32)\n\n# Test encoding\ntext = \"·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\"\nids = tokenizer.encode(text)\nprint(f\"Text: {text}\")\nprint(f\"IDs: {ids[:15]}...\")  # First 15 tokens\nprint(f\"Length: {len(ids)}\")\n\n# Test decoding\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n\n# Verify vocab size\nprint(f\"Vocab size: {len(tokenizer)}\")  # Should be 37 (4 special + 33 Georgian)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:19.272098Z","iopub.execute_input":"2026-01-24T06:09:19.272455Z","iopub.status.idle":"2026-01-24T06:09:19.291724Z","shell.execute_reply.started":"2026-01-24T06:09:19.272431Z","shell.execute_reply":"2026-01-24T06:09:19.290942Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:12.212292Z","start_time":"2026-01-23T18:56:12.205638Z"}},"outputs":[{"name":"stdout","text":"Text: ·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\nIDs: [1, 6, 4, 15, 4, 20, 35, 17, 5, 4, 2, 0, 0, 0, 0]...\nLength: 32\nDecoded: ·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\nVocab size: 37\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Works as expected.","metadata":{}},{"cell_type":"markdown","source":"Now we prepare dataset using this tokenizer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps\n\n\nclass GeorgianOCRDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root_dir: str, processor, tokenizer: GeorgianTokenizer):\n        self.df = df.reset_index(drop=True)\n        self.root_dir = root_dir\n        self.processor = processor\n        self.tokenizer = tokenizer  # custom tokenizer\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n        text = self.df.iloc[idx]['text']\n        file_path = f\"{self.root_dir}/{self.df.iloc[idx]['file_name']}\"\n\n        # Open and process image\n        img = Image.open(file_path).convert(\"RGB\")\n        w, h = img.size\n        target_size = 384\n\n        # Scale height to target_size, width proportionally\n        scale = target_size / max(w, h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)\n\n        # Pad to square\n        new_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n        offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n        new_img.paste(img, offset)\n\n        # Use Processor for Normalization\n        pixel_values = self.processor(new_img, return_tensors=\"pt\").pixel_values\n\n        # Tokenize Georgian Text\n        labels = self.tokenizer.encode(text)\n\n        # Replace padding token id with -100 so it's ignored by the loss function\n        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"labels\": torch.tensor(labels)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:19.292810Z","iopub.execute_input":"2026-01-24T06:09:19.293241Z","iopub.status.idle":"2026-01-24T06:09:19.305685Z","shell.execute_reply.started":"2026-01-24T06:09:19.293211Z","shell.execute_reply":"2026-01-24T06:09:19.305008Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:12.251528Z","start_time":"2026-01-23T18:56:12.243509Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Prepare model...\n\nwe give it our 37 tokens so model predicts only 37 possible outputs instead of original 50k.","metadata":{}},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\n\n# Create your tokenizer\ntokenizer = GeorgianTokenizer(max_length=32)\n\n# Load model and resize token embeddings\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\nmodel.decoder.resize_token_embeddings(len(tokenizer))  # Resize to 37\n\n# Configure special tokens\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:19.306659Z","iopub.execute_input":"2026-01-24T06:09:19.307061Z","iopub.status.idle":"2026-01-24T06:09:24.753124Z","shell.execute_reply.started":"2026-01-24T06:09:19.307025Z","shell.execute_reply":"2026-01-24T06:09:24.752369Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:13.496874Z","start_time":"2026-01-23T18:56:12.282557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373b310a72784700aa302431f909c47a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1841523eec4d03a4af5bced74bf17f"}},"metadata":{}},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004b5a4ffd1240bca7054cf1afbfc987"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.10, \n    random_state=42, \n    shuffle=True\n)\n\nprint(train_df[\"text\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:24.754153Z","iopub.execute_input":"2026-01-24T06:09:24.754441Z","iopub.status.idle":"2026-01-24T06:09:24.815389Z","shell.execute_reply.started":"2026-01-24T06:09:24.754414Z","shell.execute_reply":"2026-01-24T06:09:24.814696Z"},"ExecuteTime":{"end_time":"2026-01-23T18:56:13.743231Z","start_time":"2026-01-23T18:56:13.694601Z"}},"outputs":[{"name":"stdout","text":"text\n·Éì·Éê          3803\n·Éê·É†          1044\n·É†·Éù·Éõ          947\n·Éò·Éß·Éù          705\n·Éô·Éò           546\n            ... \n·Éò·É¢·Éê·Éö·Éò·É£·É†·Éò       1\n·É•·É£·É©·Éò·É°·Éê         1\n·É®·Éó·Éù·Éô           1\n·É¶·ÉÆ·Éù·Éõ·Éô·É¢·É™·Éê       1\n·É®·Éõ·Éù·Éò           1\nName: count, Length: 35172, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = GeorgianOCRDataset(train_df, data_dir, processor, tokenizer)\ntest_dataset = GeorgianOCRDataset(test_df, data_dir, processor, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")","metadata":{"ExecuteTime":{"end_time":"2026-01-23T19:05:32.492824Z","start_time":"2026-01-23T19:05:32.456745Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:24.816387Z","iopub.execute_input":"2026-01-24T06:09:24.817044Z","iopub.status.idle":"2026-01-24T06:09:24.826291Z","shell.execute_reply.started":"2026-01-24T06:09:24.817011Z","shell.execute_reply":"2026-01-24T06:09:24.825584Z"}},"outputs":[{"name":"stdout","text":"Train batches: 11307, Test batches: 1257\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Set up training","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nprint(f\"Training on: {device}\")","metadata":{"ExecuteTime":{"end_time":"2026-01-23T19:06:09.184932Z","start_time":"2026-01-23T19:06:09.151301Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T06:09:32.513103Z","iopub.execute_input":"2026-01-24T06:09:32.514153Z","iopub.status.idle":"2026-01-24T06:09:33.065720Z","shell.execute_reply.started":"2026-01-24T06:09:32.514117Z","shell.execute_reply":"2026-01-24T06:09:33.065041Z"}},"outputs":[{"name":"stdout","text":"Training on: cuda\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Validation function","metadata":{}},{"cell_type":"code","source":"from evaluate import load\nimport torch\n\n# Load the CER metric (standard for OCR)\ncer_metric = load(\"cer\")\n\ndef validate_model(\n    model: torch.nn.Module, \n    val_loader: torch.utils.data.DataLoader, \n    processor: any, \n    device: torch.device\n) -> float:\n    model.eval()\n    predictions: list[str] = []\n    references: list[str] = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Generate text from image\n            outputs = model.generate(pixel_values)\n            \n            # Convert tokens back to strings\n            pred_str = processor.batch_decode(outputs, skip_special_tokens=True)\n            \n            # Convert label tokens back to strings (ignoring -100 padding)\n            labels[labels == -100] = processor.tokenizer.pad_token_id\n            label_str = processor.batch_decode(labels, skip_special_tokens=True)\n            \n            predictions.extend(pred_str)\n            references.extend(label_str)\n    \n    # Calculate Character Error Rate\n    cer_score: float = cer_metric.compute(predictions=predictions, references=references)\n    return cer_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"def train_model(\n    model: torch.nn.Module, \n    train_loader: torch.utils.data.DataLoader, \n    optimizer: torch.optim.Optimizer, \n    device: torch.device, \n    epochs: int = 3,\n    save_every: int = 1000\n) -> None:\n    \n    for epoch in range(epochs):\n        model.train()\n        print(f\"\\n--- Starting Epoch {epoch} ---\")\n        \n        for batch_idx, batch in enumerate(train_loader):\n            try:\n                # Prepare data\n                pixel_values: torch.Tensor = batch[\"pixel_values\"].to(device)\n                labels: torch.Tensor = batch[\"labels\"].to(device)\n\n                # Forward pass\n                outputs = model(pixel_values=pixel_values, labels=labels)\n                loss: torch.Tensor = outputs.loss\n\n                # Backward pass\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                # Logging\n                if batch_idx % 100 == 0:\n                    print(f\"Epoch: {epoch} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n\n                # 3. Checkpointing Logic\n                if batch_idx > 0 and batch_idx % save_every == 0:\n                    checkpoint_name: str = f\"trocr_georgian_e{epoch}_s{batch_idx}.pt\"\n                    checkpoint_path: str = os.path.join(\"/kaggle/working/\", checkpoint_name)\n                    \n                    torch.save({\n                        'epoch': epoch,\n                        'batch_idx': batch_idx,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'loss': loss.item(),\n                    }, checkpoint_path)\n                    print(f\"üíæ Saved checkpoint to {checkpoint_path}\")\n\n            except RuntimeError as e:\n                # 4. The OOM Shield\n                if \"out of memory\" in str(e).lower():\n                    print(f\"GPU OOM detected at batch {batch_idx}. Cleaning memory and skipping...\")\n                    \n                    # Manually clear all variables that could be holding GPU references\n                    if 'outputs' in locals(): del outputs\n                    if 'loss' in locals(): del loss\n                    del pixel_values, labels\n                    \n                    optimizer.zero_grad(set_to_none=True) # Heavy-duty grad clearing\n                    gc.collect()                          # Python garbage collection\n                    torch.cuda.empty_cache()              # Clear NVIDIA cache\n                    continue \n                else:\n                    raise e # Re-raise if it's a different error\n\n        # At the end of the epoch, check accuracy\n        print(f\"üìä Running Validation for Epoch {epoch}...\")\n        current_cer: float = validate_model(model, test_loader, processor, device)\n        \n        print(f\"‚úÖ Epoch {epoch} Results:\")\n        print(f\"   Character Error Rate (CER): {current_cer:.4f}\")\n        print(f\"   (Translation: {100 - (current_cer*100):.2f}% character accuracy)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving the fine-tuned model","metadata":{}}]}