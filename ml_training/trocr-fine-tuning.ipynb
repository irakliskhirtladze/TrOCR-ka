{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 14495532,
     "sourceType": "datasetVersion",
     "datasetId": 9220946
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom pathlib import Path\n\nBASE_DIR = Path(\".\")\n\n\ndef check_env() -> str:\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n        print(\"Running on Kaggle\")\n        return \"kaggle\"\n    else:\n        print(\"Running locally\")\n        return \"local\"\n\n\nENV = check_env()\n\nif ENV == \"kaggle\":\n    data_dir = Path(\"/kaggle/input/ka-ocr\")\nelse:\n    data_dir = BASE_DIR / \"data\"\n\nprint(f\"\\nDataset contents in {data_dir}:\")\nfor item in data_dir.iterdir():\n    print(f\"{item.name}\")",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:50.759649Z",
     "iopub.execute_input": "2026-01-24T06:08:50.759996Z",
     "iopub.status.idle": "2026-01-24T06:08:50.771919Z",
     "shell.execute_reply.started": "2026-01-24T06:08:50.759936Z",
     "shell.execute_reply": "2026-01-24T06:08:50.771214Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:51.005267Z",
     "start_time": "2026-01-24T14:19:50.996658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "Dataset contents in data:\n",
      ".cache\n",
      "3d_unicode\n",
      "alkroundedmtav-medium\n",
      "alkroundednusx-medium\n",
      "ar-archy-regular\n",
      "arial_geo\n",
      "arial_geo-bold\n",
      "arial_geo-bold-italic\n",
      "arial_geo-italic\n",
      "bpg_algeti\n",
      "bpg_algeti_compact\n",
      "bpg_arial_2009\n",
      "bpg_boxo\n",
      "bpg_boxo-boxo\n",
      "bpg_classic_medium\n",
      "bpg_dedaena\n",
      "bpg_dedaena_nonblock\n",
      "bpg_excelsior_caps_dejavu_2010\n",
      "bpg_excelsior_dejavu_2010\n",
      "bpg_extrasquare_2009\n",
      "bpg_extrasquare_mtavruli_2009\n",
      "bpg_glaho\n",
      "bpg_glaho_2008\n",
      "bpg_glaho_arial\n",
      "bpg_glaho_bold\n",
      "bpg_glaho_sylfaen\n",
      "bpg_glaho_traditional\n",
      "bpg_ingiri_2008\n",
      "bpg_irubaqidze\n",
      "bpg_mrgvlovani_caps_2010\n",
      "bpg_nino_elite_exp\n",
      "bpg_nino_elite_ultra\n",
      "bpg_nino_elite_ultra_caps\n",
      "bpg_nino_medium_caps\n",
      "bpg_nino_mtavruli_bold\n",
      "bpg_nino_mtavruli_book\n",
      "bpg_nino_mtavruli_normal\n",
      "bpg_no9\n",
      "bpg_nostalgia\n",
      "bpg_paata\n",
      "bpg_paata_caps\n",
      "bpg_paata_cond\n",
      "bpg_paata_cond_caps\n",
      "bpg_paata_exp\n",
      "bpg_phone_sans_bold\n",
      "bpg_phone_sans_bold_italic\n",
      "bpg_phone_sans_italic\n",
      "bpg_quadrosquare_2009\n",
      "bpg_rioni\n",
      "bpg_rioni_contrast\n",
      "bpg_rioni_vera\n",
      "bpg_sans_2008\n",
      "bpg_serif_2008\n",
      "bpg_square_2009\n",
      "bpg_supersquare_2009\n",
      "bpg_ucnobi\n",
      "bpg_venuri_2010\n",
      "fixedsys_excelsior\n",
      "gf_aisi_nus-bold-italic\n",
      "gf_aisi_nus_medium-medium-italic\n",
      "gugeshashvili_slfn_2\n",
      "ka_literaturuli\n",
      "ka_lortkipanidze\n",
      "literaturulitt\n",
      "metadata.csv\n",
      "mg_bitneon\n",
      "mg_bitneon_chaos\n",
      "mg_niniko\n",
      "NotoSansGeorgian\n",
      "version.txt\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "# Explore data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:50.773360Z",
     "iopub.execute_input": "2026-01-24T06:08:50.773630Z",
     "iopub.status.idle": "2026-01-24T06:08:51.077798Z",
     "shell.execute_reply.started": "2026-01-24T06:08:50.773603Z",
     "shell.execute_reply": "2026-01-24T06:08:51.077183Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:51.078145Z",
     "start_time": "2026-01-24T14:19:51.074590Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(data_dir/\"metadata.csv\")\nprint(df.head())\nprint(df.tail())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:51.078753Z",
     "iopub.execute_input": "2026-01-24T06:08:51.079129Z",
     "iopub.status.idle": "2026-01-24T06:08:51.280758Z",
     "shell.execute_reply.started": "2026-01-24T06:08:51.079093Z",
     "shell.execute_reply": "2026-01-24T06:08:51.279910Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:51.264012Z",
     "start_time": "2026-01-24T14:19:51.091032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        file_name      text\n",
      "0  3d_unicode/3d_unicode_0000.png        ·É†·Éê\n",
      "1  3d_unicode/3d_unicode_0001.png  ·É¨·Éß·Éì·Éî·Éë·Éù·Éì·Éê\n",
      "2  3d_unicode/3d_unicode_0002.png   ·É≠·ÉØ·Éú·É¨·É§·É£·É•\n",
      "3  3d_unicode/3d_unicode_0003.png    ·É¨·Éõ·Éò·Éú·Éì·Éê\n",
      "4  3d_unicode/3d_unicode_0004.png     ·ÉØ·Éò·Éö·Éì·Éù\n",
      "                                         file_name         text\n",
      "100495  NotoSansGeorgian/NotoSansGeorgian_1495.png     ·ÉÆ·Éó·Éï·É∞·Éü·É®·ÉÆ·É®\n",
      "100496  NotoSansGeorgian/NotoSansGeorgian_1496.png       ·É†·Éù·Éõ·Éö·Éò·É°\n",
      "100497  NotoSansGeorgian/NotoSansGeorgian_1497.png  ·É®·Éî·Éõ·Éù·É†·É©·Éî·Éú·Éò·Éö·Éò\n",
      "100498  NotoSansGeorgian/NotoSansGeorgian_1498.png    ·Éú·Éî·Éù·Éö·Éò·Éó·É£·É†·Éò\n",
      "100499  NotoSansGeorgian/NotoSansGeorgian_1499.png         ·É£·É™·ÉÆ·Éù\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "print(df[\"text\"].value_counts())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:51.281894Z",
     "iopub.execute_input": "2026-01-24T06:08:51.282514Z",
     "iopub.status.idle": "2026-01-24T06:08:51.315500Z",
     "shell.execute_reply.started": "2026-01-24T06:08:51.282486Z",
     "shell.execute_reply": "2026-01-24T06:08:51.314524Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:51.305887Z",
     "start_time": "2026-01-24T14:19:51.277591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "·Éì·Éê              4205\n",
      "·Éê·É†              1155\n",
      "·É†·Éù·Éõ             1044\n",
      "·Éò·Éß·Éù              785\n",
      "·Éô·Éò               612\n",
      "                ... \n",
      "·É§·Éù·Éì·É©               1\n",
      "·Éï·ÉØ·Éú·É™               1\n",
      "·É¢·Éî·Éõ·Éû·Éò              1\n",
      "·É∞·Éò·Éë·Éò·É¨·Éú·Éü·É™·É≠·Éü·É•·Éò       1\n",
      "·ÉØ·Éû·É≠·Éù·ÉØ·Éí             1\n",
      "Name: count, Length: 37994, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "# Check text length variations\ndf[\"text_len\"] = df[\"text\"].str.len()\nprint(df[\"text_len\"].describe())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:51.317706Z",
     "iopub.execute_input": "2026-01-24T06:08:51.318341Z",
     "iopub.status.idle": "2026-01-24T06:08:51.363511Z",
     "shell.execute_reply.started": "2026-01-24T06:08:51.318314Z",
     "shell.execute_reply": "2026-01-24T06:08:51.362557Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:51.366645Z",
     "start_time": "2026-01-24T14:19:51.324256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100500.000000\n",
      "mean          6.396020\n",
      "std           2.979581\n",
      "min           2.000000\n",
      "25%           4.000000\n",
      "50%           6.000000\n",
      "75%           8.000000\n",
      "max          30.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare dataset and tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Checking if trocr already support tokenization for Georgian",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import TrOCRProcessor\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n\n# Test Georgian tokenization\ntest_text = \"·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\"\ntokens = processor.tokenizer.tokenize(test_text)\nprint(tokens)  # If you see lots of <unk> or weird splits, you need a custom tokenizer",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:08:51.364693Z",
     "iopub.execute_input": "2026-01-24T06:08:51.365460Z",
     "iopub.status.idle": "2026-01-24T06:09:19.260228Z",
     "shell.execute_reply.started": "2026-01-24T06:08:51.365431Z",
     "shell.execute_reply": "2026-01-24T06:09:19.259422Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:53.588831Z",
     "start_time": "2026-01-24T14:19:51.411153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['√°', 'ƒ•', 'ƒ¥', '√°', 'ƒ•', 'ƒ≤', '√°', 'ƒ•', 'ƒΩ', '√°', 'ƒ•', 'ƒ≤', '√°', 'ƒ•', '≈Ç', '√°', 'ƒ•', '¬Ø', '√°', 'ƒ•', 'ƒø', '√°', 'ƒ•', 'ƒ≥', '√°', 'ƒ•', 'ƒ≤']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "That's not what we need, so we'll create custom, character-based tokenizer.\n\nThe model predicts next token, in this case token represents char, not a word.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GeorgianTokenizer:\n    def __init__(self, max_length: int = 32):\n        # Special tokens\n        self.pad_token = \"<pad>\"\n        self.bos_token = \"<s>\"      # beginning of sequence\n        self.eos_token = \"</s>\"     # end of sequence\n        self.unk_token = \"<unk>\"    # unknown character\n\n        # Georgian alphabet (33 letters)\n        self.georgian_chars = \"·Éê·Éë·Éí·Éì·Éî·Éï·Éñ·Éó·Éò·Éô·Éö·Éõ·Éú·Éù·Éû·Éü·É†·É°·É¢·É£·É§·É•·É¶·Éß·É®·É©·É™·É´·É¨·É≠·ÉÆ·ÉØ·É∞\"\n\n        # Build vocabulary: special tokens + Georgian characters\n        self.vocab = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n        self.vocab.extend(list(self.georgian_chars))\n\n        # Create mappings\n        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n\n        # Token IDs for special tokens\n        self.pad_token_id = 0\n        self.bos_token_id = 1\n        self.eos_token_id = 2\n        self.unk_token_id = 3\n\n        self.max_length = max_length\n\n    def encode(self, text: str, padding: bool = True) -> list[int]:\n        \"\"\"Convert Georgian text to token IDs.\"\"\"\n        # Start with BOS token\n        ids = [self.bos_token_id]\n\n        # Convert each character\n        for char in text:\n            ids.append(self.char_to_id.get(char, self.unk_token_id))\n\n        # Add EOS token\n        ids.append(self.eos_token_id)\n\n        # Truncate if too long\n        if len(ids) > self.max_length:\n            ids = ids[:self.max_length - 1] + [self.eos_token_id]\n\n        # Pad if needed\n        if padding:\n            ids.extend([self.pad_token_id] * (self.max_length - len(ids)))\n\n        return ids\n\n    def decode(self, ids: list[int]) -> str:\n        \"\"\"Convert token IDs back to text.\"\"\"\n        chars = []\n        for id in ids:\n            if id in (self.pad_token_id, self.bos_token_id, self.eos_token_id):\n                continue\n            chars.append(self.id_to_char.get(id, \"\"))\n        return \"\".join(chars)\n\n    def __len__(self):\n        return len(self.vocab)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:19.261418Z",
     "iopub.execute_input": "2026-01-24T06:09:19.262468Z",
     "iopub.status.idle": "2026-01-24T06:09:19.271144Z",
     "shell.execute_reply.started": "2026-01-24T06:09:19.262425Z",
     "shell.execute_reply": "2026-01-24T06:09:19.270480Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:53.796785Z",
     "start_time": "2026-01-24T14:19:53.788945Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": "Test tokenization again",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = GeorgianTokenizer(max_length=32)\n\n# Test encoding\ntext = \"·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\"\nids = tokenizer.encode(text)\nprint(f\"Text: {text}\")\nprint(f\"IDs: {ids[:15]}...\")  # First 15 tokens\nprint(f\"Length: {len(ids)}\")\n\n# Test decoding\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n\n# Verify vocab size\nprint(f\"Vocab size: {len(tokenizer)}\")  # Should be 37 (4 special + 33 Georgian)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:19.272098Z",
     "iopub.execute_input": "2026-01-24T06:09:19.272455Z",
     "iopub.status.idle": "2026-01-24T06:09:19.291724Z",
     "shell.execute_reply.started": "2026-01-24T06:09:19.272431Z",
     "shell.execute_reply": "2026-01-24T06:09:19.290942Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:53.811258Z",
     "start_time": "2026-01-24T14:19:53.804465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\n",
      "IDs: [1, 6, 4, 15, 4, 20, 35, 17, 5, 4, 2, 0, 0, 0, 0]...\n",
      "Length: 32\n",
      "Decoded: ·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê\n",
      "Vocab size: 37\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "Works as expected.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Now we prepare dataset using this tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps\n\n\nclass GeorgianOCRDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root_dir: str, processor, tokenizer: GeorgianTokenizer):\n        self.df = df.reset_index(drop=True)\n        self.root_dir = root_dir\n        self.processor = processor\n        self.tokenizer = tokenizer  # custom tokenizer\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n        text = self.df.iloc[idx]['text']\n        file_path = f\"{self.root_dir}/{self.df.iloc[idx]['file_name']}\"\n\n        # Open and process image\n        img = Image.open(file_path).convert(\"RGB\")\n        w, h = img.size\n        target_size = 384\n\n        # Scale height to target_size, width proportionally\n        scale = target_size / max(w, h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)\n\n        # Pad to square\n        new_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n        offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n        new_img.paste(img, offset)\n\n        # Use Processor for Normalization\n        pixel_values = self.processor(new_img, return_tensors=\"pt\").pixel_values\n\n        # Tokenize Georgian Text\n        labels = self.tokenizer.encode(text)\n\n        # Replace padding token id with -100 so it's ignored by the loss function\n        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"labels\": torch.tensor(labels)\n        }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:19.292810Z",
     "iopub.execute_input": "2026-01-24T06:09:19.293241Z",
     "iopub.status.idle": "2026-01-24T06:09:19.305685Z",
     "shell.execute_reply.started": "2026-01-24T06:09:19.293211Z",
     "shell.execute_reply": "2026-01-24T06:09:19.305008Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:53.858440Z",
     "start_time": "2026-01-24T14:19:53.850353Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "Prepare model...\n\nwe give it our 37 tokens so model predicts only 37 possible outputs instead of original 50k.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import VisionEncoderDecoderModel\n\n# Create your tokenizer\ntokenizer = GeorgianTokenizer(max_length=32)\n\n# Load model and resize token embeddings\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\nmodel.decoder.resize_token_embeddings(len(tokenizer))  # Resize to 37\n\n# Configure special tokens\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:19.306659Z",
     "iopub.execute_input": "2026-01-24T06:09:19.307061Z",
     "iopub.status.idle": "2026-01-24T06:09:24.753124Z",
     "shell.execute_reply.started": "2026-01-24T06:09:19.307025Z",
     "shell.execute_reply": "2026-01-24T06:09:24.752369Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:55.094615Z",
     "start_time": "2026-01-24T14:19:53.885821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "# Train test split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.10, \n    random_state=42, \n    shuffle=True\n)\n\nprint(train_df[\"text\"].value_counts())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:24.754153Z",
     "iopub.execute_input": "2026-01-24T06:09:24.754441Z",
     "iopub.status.idle": "2026-01-24T06:09:24.815389Z",
     "shell.execute_reply.started": "2026-01-24T06:09:24.754414Z",
     "shell.execute_reply": "2026-01-24T06:09:24.814696Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:55.370344Z",
     "start_time": "2026-01-24T14:19:55.325792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "·Éì·Éê               3803\n",
      "·Éê·É†               1044\n",
      "·É†·Éù·Éõ               947\n",
      "·Éò·Éß·Éù               705\n",
      "·Éô·Éò                546\n",
      "                 ... \n",
      "·É°·Éö·Éí·É¨·É†·Éú·É£·É´·É¨·Éê·É°·Éì        1\n",
      "·Éö·Éë·Éî·Éï·É¨·Éì·É™             1\n",
      "+995549397648       1\n",
      "·É°·Éò·Éê·Éõ·Éù·Éï·Éú·Éî·Éë·Éê·É°·Éê·É™       1\n",
      "·Éñ·Éî·Éö·Éî·Éú·É°·Éô·Éò·É°           1\n",
      "Name: count, Length: 35172, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": "# Dataloaders",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader\n\ntrain_dataset = GeorgianOCRDataset(train_df, data_dir, processor, tokenizer)\ntest_dataset = GeorgianOCRDataset(test_df, data_dir, processor, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:24.816387Z",
     "iopub.execute_input": "2026-01-24T06:09:24.817044Z",
     "iopub.status.idle": "2026-01-24T06:09:24.826291Z",
     "shell.execute_reply.started": "2026-01-24T06:09:24.817011Z",
     "shell.execute_reply": "2026-01-24T06:09:24.825584Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:55.401435Z",
     "start_time": "2026-01-24T14:19:55.382486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 11307, Test batches: 1257\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": "# Set up training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.optim import AdamW\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nprint(f\"Training on: {device}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-24T06:09:32.513103Z",
     "iopub.execute_input": "2026-01-24T06:09:32.514153Z",
     "iopub.status.idle": "2026-01-24T06:09:33.065720Z",
     "shell.execute_reply.started": "2026-01-24T06:09:32.514117Z",
     "shell.execute_reply": "2026-01-24T06:09:33.065041Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:55.442805Z",
     "start_time": "2026-01-24T14:19:55.424053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": "# Validation function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load the CER metric (standard for OCR)\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "def validate_model(\n",
    "    model: torch.nn.Module, \n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    tokenizer: GeorgianTokenizer,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    predictions: list[str] = []\n",
    "    references: list[str] = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Generate text from image\n",
    "            outputs = model.generate(pixel_values)\n",
    "            \n",
    "            # Convert tokens back to strings\n",
    "            pred_str = [tokenizer.decode(ids.tolist()) for ids in outputs]\n",
    "            \n",
    "            # Convert label tokens back to strings (ignoring -100 padding)\n",
    "            labels[labels == -100] = tokenizer.pad_token_id\n",
    "            label_str = [tokenizer.decode(ids.tolist()) for ids in labels]\n",
    "            \n",
    "            predictions.extend(pred_str)\n",
    "            references.extend(label_str)\n",
    "    \n",
    "    # Calculate Character Error Rate\n",
    "    cer_score: float = cer_metric.compute(predictions=predictions, references=references)\n",
    "    return cer_score"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:57.684738Z",
     "start_time": "2026-01-24T14:19:55.461161Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "# Training loop",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: torch.nn.Module, \n",
    "    train_loader: torch.utils.data.DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: torch.device, \n",
    "    epochs: int = 3,\n",
    "    save_every: int = 1000\n",
    ") -> None:\n",
    "\n",
    "    # Determine output directory based on environment\n",
    "    if ENV == \"kaggle\":\n",
    "        output_dir = Path(\"/kaggle/working\")\n",
    "    else:\n",
    "        output_dir = BASE_DIR / \"checkpoints\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"\\n--- Starting Epoch {epoch} ---\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                # Prepare data\n",
    "                pixel_values: torch.Tensor = batch[\"pixel_values\"].to(device)\n",
    "                labels: torch.Tensor = batch[\"labels\"].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss: torch.Tensor = outputs.loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Logging\n",
    "                if batch_idx % 100 == 0:\n",
    "                    print(f\"Epoch: {epoch} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # 3. Checkpointing Logic\n",
    "                if batch_idx > 0 and batch_idx % save_every == 0:\n",
    "                    checkpoint_name: str = f\"trocr_georgian_e{epoch}_s{batch_idx}.pt\"\n",
    "                    checkpoint_path = output_dir / checkpoint_name\n",
    "                    \n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'batch_idx': batch_idx,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                    }, checkpoint_path)\n",
    "                    print(f\"üíæ Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # 4. The OOM Shield\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"GPU OOM detected at batch {batch_idx}. Cleaning memory and skipping...\")\n",
    "                    \n",
    "                    # Manually clear all variables that could be holding GPU references\n",
    "                    if 'outputs' in locals(): del outputs\n",
    "                    if 'loss' in locals(): del loss\n",
    "                    del pixel_values, labels\n",
    "                    \n",
    "                    optimizer.zero_grad(set_to_none=True) # Heavy-duty grad clearing\n",
    "                    gc.collect()                          # Python garbage collection\n",
    "                    torch.cuda.empty_cache()              # Clear NVIDIA cache\n",
    "                    continue \n",
    "                else:\n",
    "                    raise e # Re-raise if it's a different error\n",
    "\n",
    "        # At the end of the epoch, check accuracy\n",
    "        print(f\"üìä Running Validation for Epoch {epoch}...\")\n",
    "        current_cer: float = validate_model(model, test_loader, tokenizer, device)\n",
    "        \n",
    "        print(f\"‚úÖ Epoch {epoch} Results:\")\n",
    "        print(f\"   Character Error Rate (CER): {current_cer:.4f}\")\n",
    "        print(f\"   (Translation: {100 - (current_cer*100):.2f}% character accuracy)\")"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T14:19:57.863474Z",
     "start_time": "2026-01-24T14:19:57.852585Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T14:29:34.086669Z",
     "start_time": "2026-01-24T14:23:35.147964Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, optimizer, device, epochs=3, save_every=1000)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Epoch 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 0/11307 | Loss: 8.0716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_every\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 31\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, optimizer, device, epochs, save_every)\u001B[39m\n\u001B[32m     28\u001B[39m labels: torch.Tensor = batch[\u001B[33m\"\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m\"\u001B[39m].to(device)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m loss: torch.Tensor = outputs.loss\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:525\u001B[39m, in \u001B[36mVisionEncoderDecoderModel.forward\u001B[39m\u001B[34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001B[39m\n\u001B[32m    522\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m pixel_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    523\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mYou have to specify pixel_values\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m525\u001B[39m     encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    526\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    527\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    528\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    529\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    530\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs_encoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    531\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    532\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    533\u001B[39m     encoder_outputs = BaseModelOutput(*encoder_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1072\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1069\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m   1071\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1072\u001B[39m     outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1073\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[32m   1074\u001B[39m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[32m   1075\u001B[39m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[32m   1076\u001B[39m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[32m   1077\u001B[39m     kwargs_without_recordable = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:485\u001B[39m, in \u001B[36mViTModel.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, head_mask, interpolate_pos_encoding, **kwargs)\u001B[39m\n\u001B[32m    479\u001B[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001B[32m    481\u001B[39m embedding_output = \u001B[38;5;28mself\u001B[39m.embeddings(\n\u001B[32m    482\u001B[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001B[32m    483\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m485\u001B[39m encoder_outputs: BaseModelOutput = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    487\u001B[39m sequence_output = encoder_outputs.last_hidden_state\n\u001B[32m    488\u001B[39m sequence_output = \u001B[38;5;28mself\u001B[39m.layernorm(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:368\u001B[39m, in \u001B[36mViTEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, layer_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.layer):\n\u001B[32m    367\u001B[39m     layer_head_mask = head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m     hidden_states = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutput(last_hidden_state=hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:350\u001B[39m, in \u001B[36mViTLayer.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask)\u001B[39m\n\u001B[32m    348\u001B[39m \u001B[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001B[39;00m\n\u001B[32m    349\u001B[39m layer_output = \u001B[38;5;28mself\u001B[39m.layernorm_after(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m350\u001B[39m layer_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# second residual connection is done here\u001B[39;00m\n\u001B[32m    353\u001B[39m layer_output = \u001B[38;5;28mself\u001B[39m.output(layer_output, hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:310\u001B[39m, in \u001B[36mViTIntermediate.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    309\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.intermediate_act_fn(hidden_states)\n\u001B[32m    312\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "# Saving the fine-tuned model",
   "metadata": {}
  }
 ]
}
