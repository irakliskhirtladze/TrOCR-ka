{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 14495532,
     "sourceType": "datasetVersion",
     "datasetId": 9220946
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def check_env() -> str:\n",
    "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "        print(\"Running on Kaggle\")\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        print(\"Running locally\")\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = check_env()\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    data_dir = Path(\"/kaggle/input/ka-ocr\")\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    data_dir = Path(\"./data\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hf_repo = os.getenv(\"HF_DATASET_REPO\")\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "    if not hf_repo:\n",
    "        raise ValueError(\"HF_DATASET_REPO not set in .env\")\n",
    "\n",
    "    # Download with automatic caching - skips if local matches remote (etag-based)\n",
    "    zip_path = hf_hub_download(\n",
    "        repo_id=hf_repo,\n",
    "        filename=\"ka-ocr.zip\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=hf_token,\n",
    "        local_dir=data_dir,\n",
    "    )\n",
    "\n",
    "    # Extract only if not already extracted OR if zip is newer than extraction\n",
    "    extract_marker = data_dir / \".extracted\"\n",
    "    zip_file = Path(zip_path)\n",
    "    needs_extract = (\n",
    "        not extract_marker.exists() or\n",
    "        zip_file.stat().st_mtime > extract_marker.stat().st_mtime\n",
    "    )\n",
    "\n",
    "    if needs_extract:\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        extract_marker.touch()\n",
    "        print(\"Extraction complete\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted, skipping\")\n",
    "\n",
    "print(f\"\\nDataset contents in {data_dir}:\")\n",
    "for item in data_dir.iterdir():\n",
    "    if not item.name.startswith('.') and item.name != \"ka-ocr.zip\":\n",
    "        print(f\"  {item.name}\")"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-18T11:24:11.010653Z",
     "iopub.execute_input": "2026-01-18T11:24:11.010974Z",
     "iopub.status.idle": "2026-01-18T11:24:11.019009Z",
     "shell.execute_reply.started": "2026-01-18T11:24:11.010948Z",
     "shell.execute_reply": "2026-01-18T11:24:11.018087Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-19T10:11:54.752989Z",
     "start_time": "2026-01-19T10:11:54.538098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "Dataset already extracted, skipping\n",
      "\n",
      "Dataset contents in data:\n",
      "  3d_unicode\n",
      "  alkroundedmtav-medium\n",
      "  alkroundednusx-medium\n",
      "  ar-archy-regular\n",
      "  arial_geo\n",
      "  arial_geo-bold\n",
      "  arial_geo-bold-italic\n",
      "  arial_geo-italic\n",
      "  bpg_algeti\n",
      "  bpg_algeti_compact\n",
      "  bpg_arial_2009\n",
      "  bpg_boxo\n",
      "  bpg_boxo-boxo\n",
      "  bpg_classic_medium\n",
      "  bpg_dedaena\n",
      "  bpg_dedaena_nonblock\n",
      "  bpg_excelsior_caps_dejavu_2010\n",
      "  bpg_excelsior_dejavu_2010\n",
      "  bpg_extrasquare_2009\n",
      "  bpg_extrasquare_mtavruli_2009\n",
      "  bpg_glaho\n",
      "  bpg_glaho_2008\n",
      "  bpg_glaho_arial\n",
      "  bpg_glaho_bold\n",
      "  bpg_glaho_sylfaen\n",
      "  bpg_glaho_traditional\n",
      "  bpg_ingiri_2008\n",
      "  bpg_irubaqidze\n",
      "  bpg_mrgvlovani_caps_2010\n",
      "  bpg_nino_elite_exp\n",
      "  bpg_nino_elite_ultra\n",
      "  bpg_nino_elite_ultra_caps\n",
      "  bpg_nino_medium_caps\n",
      "  bpg_nino_mtavruli_bold\n",
      "  bpg_nino_mtavruli_book\n",
      "  bpg_nino_mtavruli_normal\n",
      "  bpg_no9\n",
      "  bpg_nostalgia\n",
      "  bpg_paata\n",
      "  bpg_paata_caps\n",
      "  bpg_paata_cond\n",
      "  bpg_paata_cond_caps\n",
      "  bpg_paata_exp\n",
      "  bpg_phone_sans_bold\n",
      "  bpg_phone_sans_bold_italic\n",
      "  bpg_phone_sans_italic\n",
      "  bpg_quadrosquare_2009\n",
      "  bpg_rioni\n",
      "  bpg_rioni_contrast\n",
      "  bpg_rioni_vera\n",
      "  bpg_sans_2008\n",
      "  bpg_serif_2008\n",
      "  bpg_square_2009\n",
      "  bpg_supersquare_2009\n",
      "  bpg_ucnobi\n",
      "  bpg_venuri_2010\n",
      "  fixedsys_excelsior\n",
      "  gf_aisi_nus-bold-italic\n",
      "  gf_aisi_nus_medium-medium-italic\n",
      "  gugeshashvili_slfn_2\n",
      "  ka_literaturuli\n",
      "  ka_lortkipanidze\n",
      "  literaturulitt\n",
      "  metadata.csv\n",
      "  mg_bitneon\n",
      "  mg_bitneon_chaos\n",
      "  mg_niniko\n",
      "  NotoSansGeorgian\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explore data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:11:46.807578Z",
     "start_time": "2026-01-19T10:11:46.803495Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:12:06.759896Z",
     "start_time": "2026-01-19T10:12:06.545791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(data_dir/\"metadata.csv\")\n",
    "print(df.head())\n",
    "print(df.tail())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        file_name          text\n",
      "0  3d_unicode/3d_unicode_0000.png          ამათ\n",
      "1  3d_unicode/3d_unicode_0001.png   პარტიანკაში\n",
      "2  3d_unicode/3d_unicode_0002.png  კომენტარების\n",
      "3  3d_unicode/3d_unicode_0003.png       ფრიდრიხ\n",
      "4  3d_unicode/3d_unicode_0004.png   ცდწლოოწნწში\n",
      "                                         file_name      text\n",
      "100495  NotoSansGeorgian/NotoSansGeorgian_1495.png     რიგში\n",
      "100496  NotoSansGeorgian/NotoSansGeorgian_1496.png     ალიკა\n",
      "100497  NotoSansGeorgian/NotoSansGeorgian_1497.png      ტარს\n",
      "100498  NotoSansGeorgian/NotoSansGeorgian_1498.png     კარგი\n",
      "100499  NotoSansGeorgian/NotoSansGeorgian_1499.png  სასახლეს\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:12:09.037143Z",
     "start_time": "2026-01-19T10:12:09.009558Z"
    }
   },
   "cell_type": "code",
   "source": "print(df[\"text\"].value_counts())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "და              4221\n",
      "არ              1163\n",
      "რომ              995\n",
      "იყო              768\n",
      "კი               604\n",
      "                ... \n",
      "ფუფთწჟჯკდპბგ       1\n",
      "ოოწლღდჩქთტტ        1\n",
      "გითქვამს           1\n",
      "სიკვდილია          1\n",
      "ბგშჩდეკშჰ          1\n",
      "Name: count, Length: 38003, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:12:11.208456Z",
     "start_time": "2026-01-19T10:12:11.161262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check text length variations\n",
    "df[\"text_len\"] = df[\"text\"].str.len()\n",
    "print(df[\"text_len\"].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100500.000000\n",
      "mean          6.390886\n",
      "std           2.970310\n",
      "min           2.000000\n",
      "25%           4.000000\n",
      "50%           6.000000\n",
      "75%           8.000000\n",
      "max          24.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare images"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:12:53.982898Z",
     "start_time": "2026-01-19T10:12:13.723407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrOCRProcessor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Test Georgian tokenization\n",
    "test_text = \"გამარჯობა\"\n",
    "tokens = processor.tokenizer.tokenize(test_text)\n",
    "print(tokens)  # If you see lots of <unk> or weird splits, you need a custom tokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iskhi\\.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['á', 'ĥ', 'Ĵ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'Ľ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'ł', 'á', 'ĥ', '¯', 'á', 'ĥ', 'Ŀ', 'á', 'ĥ', 'ĳ', 'á', 'ĥ', 'Ĳ']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T13:01:03.259390Z",
     "start_time": "2026-01-19T13:01:03.228226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeorgianTokenizer:\n",
    "    def __init__(self, max_length: int = 32):\n",
    "        # Special tokens\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.bos_token = \"<s>\"      # beginning of sequence\n",
    "        self.eos_token = \"</s>\"     # end of sequence\n",
    "        self.unk_token = \"<unk>\"    # unknown character\n",
    "\n",
    "        # Georgian alphabet (33 letters)\n",
    "        self.georgian_chars = \"აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ\"\n",
    "\n",
    "        # Build vocabulary: special tokens + Georgian characters\n",
    "        self.vocab = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
    "        self.vocab.extend(list(self.georgian_chars))\n",
    "\n",
    "        # Create mappings\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "\n",
    "        # Token IDs for special tokens\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.unk_token_id = 3\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def encode(self, text: str, padding: bool = True) -> list[int]:\n",
    "        \"\"\"Convert Georgian text to token IDs.\"\"\"\n",
    "        # Start with BOS token\n",
    "        ids = [self.bos_token_id]\n",
    "\n",
    "        # Convert each character\n",
    "        for char in text:\n",
    "            ids.append(self.char_to_id.get(char, self.unk_token_id))\n",
    "\n",
    "        # Add EOS token\n",
    "        ids.append(self.eos_token_id)\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(ids) > self.max_length:\n",
    "            ids = ids[:self.max_length - 1] + [self.eos_token_id]\n",
    "\n",
    "        # Pad if needed\n",
    "        if padding:\n",
    "            ids.extend([self.pad_token_id] * (self.max_length - len(ids)))\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        chars = []\n",
    "        for id in ids:\n",
    "            if id in (self.pad_token_id, self.bos_token_id, self.eos_token_id):\n",
    "                continue\n",
    "            chars.append(self.id_to_char.get(id, \"\"))\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T13:01:06.656230Z",
     "start_time": "2026-01-19T13:01:06.649330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = GeorgianTokenizer(max_length=32)\n",
    "\n",
    "# Test encoding\n",
    "text = \"გამარჯობა\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"IDs: {ids[:15]}...\")  # First 15 tokens\n",
    "print(f\"Length: {len(ids)}\")\n",
    "\n",
    "# Test decoding\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Verify vocab size\n",
    "print(f\"Vocab size: {len(tokenizer)}\")  # Should be 37 (4 special + 33 Georgian)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: გამარჯობა\n",
      "IDs: [1, 6, 4, 15, 4, 20, 35, 17, 5, 4, 2, 0, 0, 0, 0]...\n",
      "Length: 32\n",
      "Decoded: გამარჯობა\n",
      "Vocab size: 37\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:05:49.148107Z",
     "start_time": "2026-01-19T16:05:49.130396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "class GeorgianOCRDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root_dir: str, processor, tokenizer: GeorgianTokenizer):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer  # custom tokenizer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        file_path = f\"{self.root_dir}/{self.df.iloc[idx]['file_name']}\"\n",
    "\n",
    "        # Open and process image\n",
    "        img = Image.open(file_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        target_size = 384\n",
    "\n",
    "        # Scale height to target_size, width proportionally\n",
    "        scale = target_size / max(w, h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)\n",
    "\n",
    "        # Pad to square\n",
    "        new_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n",
    "        offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n",
    "        new_img.paste(img, offset)\n",
    "\n",
    "        # Use Processor for Normalization\n",
    "        pixel_values = self.processor(new_img, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Tokenize Georgian Text\n",
    "        labels = self.tokenizer.encode(text)\n",
    "\n",
    "        # Replace padding token id with -100 so it's ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T16:57:03.033539Z",
     "start_time": "2026-01-19T16:51:28.458240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "\n",
    "# Load processor (for image processing only)\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# Create your tokenizer\n",
    "tokenizer = GeorgianTokenizer(max_length=32)\n",
    "\n",
    "# Load model and resize token embeddings\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))  # Resize to 37\n",
    "\n",
    "# Configure special tokens\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Mega\\Python\\Ka-OCR\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iskhi\\.cache\\huggingface\\hub\\models--microsoft--trocr-base-printed. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train test split"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
