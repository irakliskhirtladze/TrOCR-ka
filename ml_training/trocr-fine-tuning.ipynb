{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 14495532,
     "sourceType": "datasetVersion",
     "datasetId": 9220946
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "\n",
    "\n",
    "def check_env() -> str:\n",
    "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "        print(\"Running on Kaggle\")\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        print(\"Running locally\")\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = check_env()\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    data_dir = Path(\"/kaggle/input/ka-ocr\")\n",
    "else:\n",
    "    data_dir = BASE_DIR / \"data\"\n",
    "\n",
    "print(f\"\\nDataset contents in {data_dir}:\")\n",
    "for item in data_dir.iterdir():\n",
    "    print(f\"{item.name}\")"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:25.878146Z",
     "iopub.execute_input": "2026-01-20T13:11:25.878780Z",
     "iopub.status.idle": "2026-01-20T13:11:25.925964Z",
     "shell.execute_reply.started": "2026-01-20T13:11:25.878749Z",
     "shell.execute_reply": "2026-01-20T13:11:25.924898Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T12:24:56.442344Z",
     "start_time": "2026-01-23T12:24:56.435420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "Dataset contents in data:\n",
      ".cache\n",
      "3d_unicode\n",
      "alkroundedmtav-medium\n",
      "alkroundednusx-medium\n",
      "ar-archy-regular\n",
      "arial_geo\n",
      "arial_geo-bold\n",
      "arial_geo-bold-italic\n",
      "arial_geo-italic\n",
      "bpg_algeti\n",
      "bpg_algeti_compact\n",
      "bpg_arial_2009\n",
      "bpg_boxo\n",
      "bpg_boxo-boxo\n",
      "bpg_classic_medium\n",
      "bpg_dedaena\n",
      "bpg_dedaena_nonblock\n",
      "bpg_excelsior_caps_dejavu_2010\n",
      "bpg_excelsior_dejavu_2010\n",
      "bpg_extrasquare_2009\n",
      "bpg_extrasquare_mtavruli_2009\n",
      "bpg_glaho\n",
      "bpg_glaho_2008\n",
      "bpg_glaho_arial\n",
      "bpg_glaho_bold\n",
      "bpg_glaho_sylfaen\n",
      "bpg_glaho_traditional\n",
      "bpg_ingiri_2008\n",
      "bpg_irubaqidze\n",
      "bpg_mrgvlovani_caps_2010\n",
      "bpg_nino_elite_exp\n",
      "bpg_nino_elite_ultra\n",
      "bpg_nino_elite_ultra_caps\n",
      "bpg_nino_medium_caps\n",
      "bpg_nino_mtavruli_bold\n",
      "bpg_nino_mtavruli_book\n",
      "bpg_nino_mtavruli_normal\n",
      "bpg_no9\n",
      "bpg_nostalgia\n",
      "bpg_paata\n",
      "bpg_paata_caps\n",
      "bpg_paata_cond\n",
      "bpg_paata_cond_caps\n",
      "bpg_paata_exp\n",
      "bpg_phone_sans_bold\n",
      "bpg_phone_sans_bold_italic\n",
      "bpg_phone_sans_italic\n",
      "bpg_quadrosquare_2009\n",
      "bpg_rioni\n",
      "bpg_rioni_contrast\n",
      "bpg_rioni_vera\n",
      "bpg_sans_2008\n",
      "bpg_serif_2008\n",
      "bpg_square_2009\n",
      "bpg_supersquare_2009\n",
      "bpg_ucnobi\n",
      "bpg_venuri_2010\n",
      "fixedsys_excelsior\n",
      "gf_aisi_nus-bold-italic\n",
      "gf_aisi_nus_medium-medium-italic\n",
      "gugeshashvili_slfn_2\n",
      "ka_literaturuli\n",
      "ka_lortkipanidze\n",
      "literaturulitt\n",
      "metadata.csv\n",
      "mg_bitneon\n",
      "mg_bitneon_chaos\n",
      "mg_niniko\n",
      "NotoSansGeorgian\n",
      "version.txt\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# Explore data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.640179600Z",
     "start_time": "2026-01-19T10:11:46.803495Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:25.928197Z",
     "iopub.execute_input": "2026-01-20T13:11:25.928975Z",
     "iopub.status.idle": "2026-01-20T13:11:27.500790Z",
     "shell.execute_reply.started": "2026-01-20T13:11:25.928934Z",
     "shell.execute_reply": "2026-01-20T13:11:27.499473Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(data_dir/\"metadata.csv\")\nprint(df.head())\nprint(df.tail())",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.645367800Z",
     "start_time": "2026-01-19T10:12:06.545791Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.502627Z",
     "iopub.execute_input": "2026-01-20T13:11:27.503207Z",
     "iopub.status.idle": "2026-01-20T13:11:27.802472Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.503165Z",
     "shell.execute_reply": "2026-01-20T13:11:27.801217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "                        file_name          text\n0  3d_unicode/3d_unicode_0000.png          ამათ\n1  3d_unicode/3d_unicode_0001.png   პარტიანკაში\n2  3d_unicode/3d_unicode_0002.png  კომენტარების\n3  3d_unicode/3d_unicode_0003.png       ფრიდრიხ\n4  3d_unicode/3d_unicode_0004.png   ცდწლოოწნწში\n                                         file_name      text\n100495  NotoSansGeorgian/NotoSansGeorgian_1495.png     რიგში\n100496  NotoSansGeorgian/NotoSansGeorgian_1496.png     ალიკა\n100497  NotoSansGeorgian/NotoSansGeorgian_1497.png      ტარს\n100498  NotoSansGeorgian/NotoSansGeorgian_1498.png     კარგი\n100499  NotoSansGeorgian/NotoSansGeorgian_1499.png  სასახლეს\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "print(df[\"text\"].value_counts())",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.647911700Z",
     "start_time": "2026-01-19T10:12:09.009558Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.803840Z",
     "iopub.execute_input": "2026-01-20T13:11:27.804224Z",
     "iopub.status.idle": "2026-01-20T13:11:27.848198Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.804188Z",
     "shell.execute_reply": "2026-01-20T13:11:27.846856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "text\nდა            4221\nარ            1163\nრომ            995\nიყო            768\nკი             604\n              ... \nდააპროექტა       1\nცხრილი           1\nწულუკიძე         1\nბიჭებთან         1\nთცზრმაქ          1\nName: count, Length: 38003, dtype: int64\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# Check text length variations\ndf[\"text_len\"] = df[\"text\"].str.len()\nprint(df[\"text_len\"].describe())",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.647911700Z",
     "start_time": "2026-01-19T10:12:11.161262Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.850754Z",
     "iopub.execute_input": "2026-01-20T13:11:27.851014Z",
     "iopub.status.idle": "2026-01-20T13:11:27.932375Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.850982Z",
     "shell.execute_reply": "2026-01-20T13:11:27.931260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "count    100500.000000\nmean          6.390886\nstd           2.970310\nmin           2.000000\n25%           4.000000\n50%           6.000000\n75%           8.000000\nmax          24.000000\nName: text_len, dtype: float64\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare dataset and tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Checking if trocr already support tokenization for Georgian",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import TrOCRProcessor\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n\n# Test Georgian tokenization\ntest_text = \"გამარჯობა\"\ntokens = processor.tokenizer.tokenize(test_text)\nprint(tokens)  # If you see lots of <unk> or weird splits, you need a custom tokenizer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.648933500Z",
     "start_time": "2026-01-19T17:02:30.235564Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.933770Z",
     "iopub.execute_input": "2026-01-20T13:11:27.934453Z",
     "iopub.status.idle": "2026-01-20T13:12:07.687405Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.934416Z",
     "shell.execute_reply": "2026-01-20T13:12:07.686212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2026-01-20 13:11:43.634719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768914703.882243      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768914703.954849      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768914704.534825      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768914704.534886      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768914704.534890      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768914704.534893      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce61073a41ee4098a0bc9b8dddb4e3e2"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be81c31de6a4429dbab2dfb4c98ae006"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5353b7173bdb4759a048d08dbf18c409"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "191811e86d204c10ae56d80fb6b6f887"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5869029db1654d87ba13e5e9ae034d3a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "['á', 'ĥ', 'Ĵ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'Ľ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'ł', 'á', 'ĥ', '¯', 'á', 'ĥ', 'Ŀ', 'á', 'ĥ', 'ĳ', 'á', 'ĥ', 'Ĳ']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "That's not what we need so we'll create custom, character-based tokenizer.\n\nThe model predicts next token, in this case token represents char, not a word.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GeorgianTokenizer:\n    def __init__(self, max_length: int = 32):\n        # Special tokens\n        self.pad_token = \"<pad>\"\n        self.bos_token = \"<s>\"      # beginning of sequence\n        self.eos_token = \"</s>\"     # end of sequence\n        self.unk_token = \"<unk>\"    # unknown character\n\n        # Georgian alphabet (33 letters)\n        self.georgian_chars = \"აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ\"\n\n        # Build vocabulary: special tokens + Georgian characters\n        self.vocab = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n        self.vocab.extend(list(self.georgian_chars))\n\n        # Create mappings\n        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n\n        # Token IDs for special tokens\n        self.pad_token_id = 0\n        self.bos_token_id = 1\n        self.eos_token_id = 2\n        self.unk_token_id = 3\n\n        self.max_length = max_length\n\n    def encode(self, text: str, padding: bool = True) -> list[int]:\n        \"\"\"Convert Georgian text to token IDs.\"\"\"\n        # Start with BOS token\n        ids = [self.bos_token_id]\n\n        # Convert each character\n        for char in text:\n            ids.append(self.char_to_id.get(char, self.unk_token_id))\n\n        # Add EOS token\n        ids.append(self.eos_token_id)\n\n        # Truncate if too long\n        if len(ids) > self.max_length:\n            ids = ids[:self.max_length - 1] + [self.eos_token_id]\n\n        # Pad if needed\n        if padding:\n            ids.extend([self.pad_token_id] * (self.max_length - len(ids)))\n\n        return ids\n\n    def decode(self, ids: list[int]) -> str:\n        \"\"\"Convert token IDs back to text.\"\"\"\n        chars = []\n        for id in ids:\n            if id in (self.pad_token_id, self.bos_token_id, self.eos_token_id):\n                continue\n            chars.append(self.id_to_char.get(id, \"\"))\n        return \"\".join(chars)\n\n    def __len__(self):\n        return len(self.vocab)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.648933500Z",
     "start_time": "2026-01-19T13:01:03.228226Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.689057Z",
     "iopub.execute_input": "2026-01-20T13:12:07.689789Z",
     "iopub.status.idle": "2026-01-20T13:12:07.702648Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.689756Z",
     "shell.execute_reply": "2026-01-20T13:12:07.701223Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "Test tokenization again",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = GeorgianTokenizer(max_length=32)\n\n# Test encoding\ntext = \"გამარჯობა\"\nids = tokenizer.encode(text)\nprint(f\"Text: {text}\")\nprint(f\"IDs: {ids[:15]}...\")  # First 15 tokens\nprint(f\"Length: {len(ids)}\")\n\n# Test decoding\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n\n# Verify vocab size\nprint(f\"Vocab size: {len(tokenizer)}\")  # Should be 37 (4 special + 33 Georgian)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.648933500Z",
     "start_time": "2026-01-19T13:01:06.649330Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.704028Z",
     "iopub.execute_input": "2026-01-20T13:12:07.704377Z",
     "iopub.status.idle": "2026-01-20T13:12:07.741194Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.704351Z",
     "shell.execute_reply": "2026-01-20T13:12:07.739725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Text: გამარჯობა\nIDs: [1, 6, 4, 15, 4, 20, 35, 17, 5, 4, 2, 0, 0, 0, 0]...\nLength: 32\nDecoded: გამარჯობა\nVocab size: 37\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "Works as expected.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Now we prepare dataset using this tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps\n\n\nclass GeorgianOCRDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root_dir: str, processor, tokenizer: GeorgianTokenizer):\n        self.df = df.reset_index(drop=True)\n        self.root_dir = root_dir\n        self.processor = processor\n        self.tokenizer = tokenizer  # custom tokenizer\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n        text = self.df.iloc[idx]['text']\n        file_path = f\"{self.root_dir}/{self.df.iloc[idx]['file_name']}\"\n\n        # Open and process image\n        img = Image.open(file_path).convert(\"RGB\")\n        w, h = img.size\n        target_size = 384\n\n        # Scale height to target_size, width proportionally\n        scale = target_size / max(w, h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)\n\n        # Pad to square\n        new_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n        offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n        new_img.paste(img, offset)\n\n        # Use Processor for Normalization\n        pixel_values = self.processor(new_img, return_tensors=\"pt\").pixel_values\n\n        # Tokenize Georgian Text\n        labels = self.tokenizer.encode(text)\n\n        # Replace padding token id with -100 so it's ignored by the loss function\n        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"labels\": torch.tensor(labels)\n        }\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.655234200Z",
     "start_time": "2026-01-19T16:05:49.130396Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.742793Z",
     "iopub.execute_input": "2026-01-20T13:12:07.743248Z",
     "iopub.status.idle": "2026-01-20T13:12:07.767293Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.743205Z",
     "shell.execute_reply": "2026-01-20T13:12:07.766182Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "Prepare model...\n\nwe give it our 37 tokens so model predicts only 37 possible outputs instead of original 50k.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n\n# Load processor (for image processing only)\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n\n# Create your tokenizer\ntokenizer = GeorgianTokenizer(max_length=32)\n\n# Load model and resize token embeddings\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\nmodel.decoder.resize_token_embeddings(len(tokenizer))  # Resize to 37\n\n# Configure special tokens\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T06:35:46.656227900Z",
     "start_time": "2026-01-19T16:51:28.458240Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.768621Z",
     "iopub.execute_input": "2026-01-20T13:12:07.769192Z",
     "iopub.status.idle": "2026-01-20T13:12:15.417654Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.769142Z",
     "shell.execute_reply": "2026-01-20T13:12:15.416666Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e39fe2a19bf4312b9f1906080b4189e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3996911dc99448e897a1bbd75412aa8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af4ed7451dc144ba87362a42e92fe747"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "# Train test split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.10, \n    random_state=42, \n    shuffle=True\n)\n\nprint(train_df[\"text\"].value_counts())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:37:44.097814Z",
     "iopub.execute_input": "2026-01-20T13:37:44.098998Z",
     "iopub.status.idle": "2026-01-20T13:37:44.154832Z",
     "shell.execute_reply.started": "2026-01-20T13:37:44.098952Z",
     "shell.execute_reply": "2026-01-20T13:37:44.153900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "text\nდა          3843\nარ          1043\nრომ          896\nიყო          696\nკი           540\n            ... \nფურცლებს       1\nკლერი          1\nმოიშორა        1\nნნუხ           1\nკარვები        1\nName: count, Length: 35261, dtype: int64\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  }
 ]
}
